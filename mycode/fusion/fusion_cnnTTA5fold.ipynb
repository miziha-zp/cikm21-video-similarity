{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02bd7fe6-25d4-4f1e-9cb6-4a92cfab3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import numpy as np \n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "sys.path.append('../')\n",
    "import json\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "# from util import test_spearmanr\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import scipy \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_spearmanr(vid_embedding, annotation_file):\n",
    "    relevances, similarities = [], []\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            query, candidate, relevance = line.split()\n",
    "            if query not in vid_embedding:\n",
    "                print(f'ERROR: {query} NOT found')\n",
    "                continue\n",
    "                # raise Exception(f'ERROR: {query} NOT found')\n",
    "            if candidate not in vid_embedding:\n",
    "                print(f'ERROR: {candidate} NOT found')\n",
    "                continue\n",
    "                # raise Exception(f'ERROR: {candidate} NOT found')\n",
    "            # print('pass')\n",
    "            query_embedding = vid_embedding.get(query)\n",
    "            candidate_embedding = vid_embedding.get(candidate)\n",
    "            similarity = cosine_similarity([query_embedding], [candidate_embedding])[0][0]\n",
    "            similarities.append(similarity)\n",
    "            relevances.append(float(relevance))\n",
    "\n",
    "    spearmanr = scipy.stats.spearmanr(similarities, relevances).correlation\n",
    "    print('spearmanr:', spearmanr)\n",
    "    return spearmanr\n",
    "annotation_file = '/data03/yrqUni/Workspace/QQB/Data/pairwise/label.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e06caa0d-09ea-4acf-a48c-585a2bb8c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yrq_valid\n",
    "import pickle\n",
    "with open('/data03/yrqUni/Workspace/QQB/yrq/VAl.pickle', 'rb') as file:\n",
    "    yrq_bert_valid = pickle.load(file)\n",
    "    \n",
    "with open('/data03/yrqUni/Workspace/QQB/yrq/TEST.pickle', 'rb') as file:\n",
    "    yrq_bert_test = pickle.load(file)\n",
    "    \n",
    "with open('/data03/yrqUni/Workspace/QQB/yrq/VAl_2.pickle', 'rb') as file:\n",
    "    yrq_roberta_valid = pickle.load(file)\n",
    "    \n",
    "with open('/data03/yrqUni/Workspace/QQB/yrq/TEST_2.pickle', 'rb') as file:\n",
    "    yrq_roberta_test = pickle.load(file)\n",
    "    \n",
    "yrq_bert_valid = [yrq_bert_valid]\n",
    "yrq_bert_test = [yrq_bert_test]\n",
    "yrq_roberta_valid = [yrq_roberta_valid]\n",
    "yrq_roberta_test =[yrq_roberta_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ff2400-7906-479d-a537-445470232892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93M\t/data03/yrqUni/Workspace/QQB/yrq/TEST_2.pickle\n",
      "93M\t/data03/yrqUni/Workspace/QQB/yrq/TEST.pickle\n",
      "130M\t/data03/yrqUni/Workspace/QQB/yrq/VAl_2.pickle\n",
      "130M\t/data03/yrqUni/Workspace/QQB/yrq/VAl.pickle\n"
     ]
    }
   ],
   "source": [
    "!du -sh /data03/yrqUni/Workspace/QQB/yrq/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1480b7db-ba55-4a71-8c6a-b71716c80663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def json_loads(path):\n",
    "    with open(path, 'r') as load_f:\n",
    "        load_dict = json.load(load_f)\n",
    "    for k in load_dict:\n",
    "        load_dict[k] = np.array(load_dict[k])\n",
    "    return load_dict\n",
    "\n",
    "yrq_valid = [json_loads('/data03/yrqUni/Workspace/QQB/zp/val_emd.json')]\n",
    "yrq_test = [json_loads('/data03/yrqUni/Workspace/QQB/zp/result.json')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3384202-e3fa-42f1-94d3-7f4093b41c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87ae403-89a6-4040-8410-241fbc11ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trans2array(inp):\n",
    "#     ans = []\n",
    "#     for emb in inp:\n",
    "#         for k in emb:\n",
    "#             emb[k] = np.array(emb[k]).astype(np.float16)\n",
    "#         ans.append(emb)\n",
    "#     return ans\n",
    "\n",
    "# yrq_bert_valid = trans2array(yrq_bert_valid)\n",
    "# yrq_bert_test = trans2array(yrq_bert_test)\n",
    "# yrq_roberta_valid = trans2array(yrq_roberta_valid)\n",
    "# yrq_roberta_test = trans2array(yrq_roberta_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2bd21a-0651-4961-9bce-a9c306f54116",
   "metadata": {},
   "outputs": [],
   "source": [
    "ich_valid = joblib.load('/data03/yrqUni/Workspace/QQB/zp/ICH_valid.pkl')\n",
    "ich_test = joblib.load('/data03/yrqUni/Workspace/QQB/zp/ICH_test.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f07eb2f-d70d-477d-a145-ad9a8884b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_two_stage_file = '/data03/yrqUni/Workspace/QQB/zp/code/model2/pkl/test_bitm_vision_transdp01_bertlast_valid.pkl'\n",
    "test_two_stage_file = '/data03/yrqUni/Workspace/QQB/zp/code/model2/pkl/test_bitm_vision_transdp01_bertlast_test.pkl'\n",
    "\n",
    "baseline_valid = joblib.load(valid_two_stage_file)\n",
    "baseline_test = joblib.load(test_two_stage_file)\n",
    "\n",
    "valid_two_stage_file2 = '/data03/yrqUni/Workspace/QQB/zp/code/model2/pkl/test_robitm_vision_transdp01_bertlast_valid.pkl'\n",
    "test_two_stage_file2 = '/data03/yrqUni/Workspace/QQB/zp/code/model2/pkl/test_robitm_vision_transdp01_bertlast_test.pkl'\n",
    "\n",
    "baseline_valid2 = joblib.load(valid_two_stage_file2)\n",
    "baseline_test2 = joblib.load(test_two_stage_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa2623-7949-45f0-be0e-ee0d5c87bdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c657ae61-f3c0-4920-b96e-63ba47e83900",
   "metadata": {},
   "outputs": [],
   "source": [
    "zlh_trans_valid = joblib.load('/data03/yrqUni/Workspace/QQB/ZLH/pretrain_baseline/pkl/valid_two_stagedict_input_zlh1002.pkl')\n",
    "zlh_trans_test = joblib.load('/data03/yrqUni/Workspace/QQB/ZLH/pretrain_baseline/pkl/test_two_stagedict_input_zlh1002.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc0b585f-aed9-47d6-b774-9ac806c2f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_valid = joblib.load('/data03/yrqUni/Workspace/QQB/zp/code/baseline/pkl/roberta_valid_seq.pkl')\n",
    "roberta_test = joblib.load('/data03/yrqUni/Workspace/QQB/zp/code/baseline/pkl/roberta_test_seq.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed54216-37e7-4b51-9937-6e40cbaae497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nevery dict like:\\n{\\n    \"vid0\":np.array([1,2,3])...\\n}\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_knowledgePool_t = baseline_valid + roberta_valid + zlh_trans_valid \\\n",
    "    + baseline_valid2 + ich_valid + yrq_valid + yrq_bert_valid + yrq_roberta_valid\n",
    "test_knowledgePool_t = baseline_test + roberta_test + zlh_trans_test \\\n",
    "    + baseline_test2 + ich_test + yrq_test + yrq_bert_test + yrq_roberta_test\n",
    " \n",
    "# knowledgePool is list of dict \n",
    "'''\n",
    "every dict like:\n",
    "{\n",
    "    \"vid0\":np.array([1,2,3])...\n",
    "}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c508596e-eb4f-4a46-a7a2-0b09c9132f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "(32, 768)\n",
      "(32, 1024)\n",
      "(32, 1024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "valid_knowledgePool = []\n",
    "test_knowledgePool = []\n",
    "print(len(valid_knowledgePool_t))\n",
    "for k1,k2 in zip(valid_knowledgePool_t, test_knowledgePool_t):\n",
    "    if len(k1['2345203561710400875'].shape) == 1:\n",
    "#         score = test_spearmanr(k1, annotation_file)\n",
    "#         print(k1['2345203561710400875'].shape, score)\n",
    "#         if score < 0.6:\n",
    "#             print(\"continue\")\n",
    "#             continue\n",
    "        valid_knowledgePool.append(k1)\n",
    "        test_knowledgePool.append(k2)\n",
    "\n",
    "for k1, k2 in zip(valid_knowledgePool_t, test_knowledgePool_t):\n",
    "    if len(k1['2345203561710400875'].shape) == 2:\n",
    "        print(k1['2345203561710400875'].shape)\n",
    "        valid_knowledgePool.append(k1)\n",
    "        test_knowledgePool.append(k2)\n",
    "del valid_knowledgePool_t, test_knowledgePool_t\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f403ce5e-e01b-4868-9dce-ea241252291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345203561710400875\n"
     ]
    }
   ],
   "source": [
    "for i in valid_knowledgePool[0]:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d0189f-6ecc-4a25-bfcd-130e372e294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(256,), (256,), (256,), (10000,), (512,), (1024,), (1024,), (10000,), (10000,), (512,), (768,), (768,), (768,), (512,), (512,), (512,), (10000,), (256,), (256,), (256,), (10000,), (256,), (256,), (512,), (512,), (32, 768), (32, 1024), (32, 1024)]\n"
     ]
    }
   ],
   "source": [
    "inputszlist = [know['2345203561710400875'].shape for know in valid_knowledgePool]\n",
    "print(inputszlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "341cb334-cabb-4d45-a24a-0aa19aef491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class ValidDataSet(Dataset):\n",
    "    def __init__(self, knowledgePool, df, valid=True):\n",
    "        self.df = df\n",
    "        self.knowledgePool = knowledgePool\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = self.df.iloc[idx]\n",
    "        query, candidate, relevance = line['query'], line[\"candidate\"], line[\"relevance\"]\n",
    "\n",
    "        queryemblist = []\n",
    "        for knowledge in self.knowledgePool:\n",
    "            queryemblist.append(torch.from_numpy(knowledge[query].astype(np.float32)))\n",
    "\n",
    "        candidateemblist = []\n",
    "        for knowledge in self.knowledgePool:\n",
    "            candidateemblist.append(torch.from_numpy(knowledge[candidate].astype(np.float32)))\n",
    "\n",
    "        relevance = float(relevance)\n",
    "        return  tuple(queryemblist), tuple(candidateemblist), relevance\n",
    "            \n",
    "\n",
    "class testDataSet(Dataset):\n",
    "    def __init__(self, knowledgePool, valid=True):\n",
    "        self.knowledgePool = knowledgePool\n",
    "        self.id = [id for id in self.knowledgePool[0]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.id[idx]\n",
    "        emblist = []\n",
    "        for knowledge in self.knowledgePool:\n",
    "            emblist.append(torch.from_numpy(knowledge[id].astype(np.float32)))\n",
    "            \n",
    "        return emblist, id\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6237c28-88f6-416a-ba06-9020cb772045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00    0.363658\n",
      "1.00    0.139192\n",
      "0.50    0.069486\n",
      "0.05    0.055126\n",
      "0.10    0.038160\n",
      "0.60    0.030648\n",
      "0.15    0.030428\n",
      "0.55    0.028866\n",
      "0.65    0.028410\n",
      "0.35    0.023240\n",
      "0.40    0.023152\n",
      "0.45    0.020148\n",
      "0.85    0.019367\n",
      "0.20    0.019249\n",
      "0.90    0.019161\n",
      "0.95    0.017703\n",
      "0.70    0.017555\n",
      "0.30    0.016583\n",
      "0.25    0.015582\n",
      "0.80    0.012592\n",
      "0.75    0.011694\n",
      "Name: relevance, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>candidate</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2345203561710400875</td>\n",
       "      <td>5344606281729891758</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>759945328762912122</td>\n",
       "      <td>5560784129870034298</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2336209090533711226</td>\n",
       "      <td>8010747189096226170</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2588403135132530094</td>\n",
       "      <td>5380633252649788846</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3245934094259209594</td>\n",
       "      <td>120437073895839098</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 query            candidate  relevance\n",
       "0  2345203561710400875  5344606281729891758       1.00\n",
       "1   759945328762912122  5560784129870034298       0.85\n",
       "2  2336209090533711226  8010747189096226170       1.00\n",
       "3  2588403135132530094  5380633252649788846       0.25\n",
       "4  3245934094259209594   120437073895839098       1.00"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list = []\n",
    "candidate_list = []\n",
    "relevance_list = []\n",
    "data_path = '/data03/yrqUni/Workspace/QQB/Data'\n",
    "annotation_file = os.path.join(data_path, 'pairwise/label.tsv')\n",
    "\n",
    "with open(annotation_file, 'r') as f:\n",
    "    for line in f:\n",
    "        query, candidate, relevance = line.split()\n",
    "        query_list.append(query)\n",
    "        candidate_list.append(candidate)\n",
    "        relevance_list.append(relevance)\n",
    "\n",
    "\n",
    "valid_df = pd.DataFrame({\n",
    "    \"query\":query_list,\n",
    "    \"candidate\":candidate_list,\n",
    "    \"relevance\":relevance_list\n",
    "})\n",
    "\n",
    "valid_df['relevance'] =  valid_df['relevance'].apply(float)\n",
    "print(valid_df['relevance'].value_counts(normalize=True))\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcad9543-e80f-4de3-b477-8002be874c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZETEST = 3072\n",
    "testdataset = testDataSet(test_knowledgePool)\n",
    "test_dataloader = DataLoader(testdataset, batch_size=BATCHSIZETEST, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df1cf18c-8a6a-4e7e-a78b-5982bf84a44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanCorrelationLoss()\n",
      "28 [(256,), (256,), (256,), (10000,), (512,), (1024,), (1024,), (10000,), (10000,), (512,), (768,), (768,), (768,), (512,), (512,), (512,), (10000,), (256,), (256,), (256,), (10000,), (256,), (256,), (512,), (512,), (32, 768), (32, 1024), (32, 1024)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# ?train_test_split\n",
    "\n",
    "from KN_TEXTCNN import Network\n",
    "\n",
    "# inputszlist = [k[0] for k in inputszlist]\n",
    "\n",
    "import scipy\n",
    "from torch.nn.functional import normalize, dropout\n",
    "\n",
    "from customloss import *\n",
    "from termcolor import colored\n",
    "from sklearn.model_selection import train_test_split\n",
    "TEMP = 0.2\n",
    "ALLIN = False\n",
    "OUTSZ = 256\n",
    "LOSS = SpearmanCorrelationLoss(TEMP) \n",
    "# LOSS = WhiteMSE()\n",
    "# LOSS = wiseMSE()\n",
    "# LOSS = SentenceBertLoss(OUTSZ).cuda()\n",
    "print(LOSS)\n",
    "DEVICE = 'cuda:0'\n",
    "BATCHSIZE = int(2048*0.7)\n",
    "print(len(inputszlist), inputszlist)\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "np.random.seed(42)\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "# train_part, valid_part = train_test_split(valid_df, test_size=0.01)\n",
    "\n",
    "y = valid_df['relevance']\n",
    "groups = valid_df['candidate']\n",
    "train_part, valid_part = 0, 0 \n",
    "\n",
    "\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fd91d55-71ff-4524-9639-007a7aeecbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def query_expansion(df):\n",
    "    '''\n",
    "    1. (A, B) = 1 (B, C) = 1 ==> (A, C) = 1\n",
    "    2. (A, B) = 1 (B, C) = 0 ==> (A, C) = 0\n",
    "    '''\n",
    "    \n",
    "    relevance_df = df[df['relevance'] == 1.0]\n",
    "    print('relevance_df:', relevance_df.shape)\n",
    "    notrelevance_df = df[df['relevance'] == 0.0]\n",
    "    print('notrelevance_df:', notrelevance_df.shape)\n",
    "    # (A, B) = 1 (B, C) = 1 ==> (A, C) = 1\n",
    "    query_list = []\n",
    "    candidate_list = []\n",
    "    relevance_list = []\n",
    "    \n",
    "    rule1_count = 0\n",
    "    for A in tqdm(relevance_df.candidate.values):\n",
    "        # find very B which (A, B) = 1\n",
    "        B_set = set(relevance_df[relevance_df['candidate'] == A][\"query\"]) | \\\n",
    "            set(relevance_df[relevance_df['query'] == A][\"candidate\"])\n",
    "        \n",
    "        for B in B_set:\n",
    "            # find very C which (B, C) = 1\n",
    "            C_set = set(relevance_df[relevance_df['candidate'] == B]['query']) | \\\n",
    "                set(relevance_df[relevance_df['query'] == B]['candidate'])\n",
    "            for C in C_set:\n",
    "                if C != A:\n",
    "                    rule1_count += 1\n",
    "                    query_list.append(C)\n",
    "                    candidate_list.append(A)\n",
    "                    relevance_list.append(1.0)\n",
    "    print('rule1_count:', rule1_count)\n",
    "    rule1_df = pd.DataFrame({\n",
    "        \"query\":query_list,\n",
    "        \"candidate\":candidate_list,\n",
    "        \"relevance\":relevance_list\n",
    "    })\n",
    "    query_list = []\n",
    "    candidate_list = []\n",
    "    relevance_list = []\n",
    "    \n",
    "    rule2_count = 0\n",
    "    for A in tqdm(relevance_df.candidate.values):\n",
    "        # find very B which (A, B) = 1\n",
    "        B_set = set(relevance_df[relevance_df['candidate'] == A][\"query\"]) | \\\n",
    "            set(relevance_df[relevance_df['query'] == A][\"candidate\"])\n",
    "        for B in B_set:\n",
    "            # find very C which (B, C) = 0\n",
    "            C_set = set(notrelevance_df[notrelevance_df['candidate'] == B]['query']) | \\\n",
    "                set(notrelevance_df[notrelevance_df['query'] == B]['candidate'])\n",
    "            for C in C_set:\n",
    "                if C != A:\n",
    "                    rule2_count += 1\n",
    "                    query_list.append(C)\n",
    "                    candidate_list.append(A)\n",
    "                    relevance_list.append(0.0)\n",
    "    print('rule2_count:', rule2_count)\n",
    "    rule2_df = pd.DataFrame({\n",
    "        \"query\":query_list,\n",
    "        \"candidate\":candidate_list,\n",
    "        \"relevance\":relevance_list\n",
    "    })\n",
    "    valid_df = pd.concat([rule1_df.sample(30000), rule2_df, df], axis=0)#.sample(fraq=1)\n",
    "    return valid_df\n",
    "            \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bc7d9aa-c6f6-40e2-a897-5e8690c0394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inference_one_epoch(test_dataloader, NET):\n",
    "    vid_embedding = {}\n",
    "    NET.eval()\n",
    "    for data1, ids in test_dataloader:\n",
    "        data1 = tuple([data.to(DEVICE) for data in data1])\n",
    "        with torch.no_grad():\n",
    "            out1 = NET(data1).detach().cpu().numpy().astype(np.float16)\n",
    "        for vid, embedding in zip(ids, out1):\n",
    "            vid_embedding[vid] = embedding.tolist()\n",
    "    return vid_embedding\n",
    "\n",
    "\n",
    "def merge_embedding(vid_embedding_list):\n",
    "    info = f'total {len(vid_embedding_list)} embeddings'\n",
    "    print(colored(info, 'blue'))\n",
    "    embedding_number = len(vid_embedding_list)\n",
    "    final_embedding = {}\n",
    "    all_vids = [key for key in vid_embedding_list[0]] \n",
    "    from tqdm import tqdm\n",
    "    for vid in tqdm(all_vids):\n",
    "        ans = 0\n",
    "        for embedding in vid_embedding_list:\n",
    "            ans += np.array(embedding[vid]) #/ embedding_number\n",
    "        final_embedding[vid] = ans.astype(np.float16).tolist()\n",
    "    return final_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee1fa62-c8bc-47f5-840e-9492d119f843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54319, 3)\n",
      "(13580, 3)\n",
      "tensor([0.0000, 0.9000, 0.0000,  ..., 0.8000, 0.0500, 1.0000], device='cuda:0')\n",
      "epoch:0 \t , step:0\t loss: -0.5743523240089417\n",
      "\u001b[34mepoch:0\t spearmanr:0.7929489776222366 \tloss_epoch:-0.8192177807029924\u001b[0m\n",
      "tensor([0.2500, 0.0500, 0.0000,  ..., 0.0000, 0.6500, 0.7000], device='cuda:0')\n",
      "valid epoch:0 \t , step:0\t loss: -0.8446042537689209\n",
      "\u001b[32mvalid ::: epoch:0\t spearmanr:0.8317599724967318 \tloss_epoch:-0.8414620935916901\u001b[0m\n",
      "tensor([1.0000, 0.0000, 0.1000,  ..., 0.1500, 0.0500, 0.3500], device='cuda:0')\n",
      "epoch:1 \t , step:0\t loss: -0.945759117603302\n",
      "\u001b[34mepoch:1\t spearmanr:0.909053062170239 \tloss_epoch:-0.9475517021982294\u001b[0m\n",
      "tensor([0.2500, 0.0500, 0.0000,  ..., 0.0000, 0.6500, 0.7000], device='cuda:0')\n",
      "valid epoch:1 \t , step:0\t loss: -0.8622879981994629\n",
      "\u001b[32mvalid ::: epoch:1\t spearmanr:0.8466848866267687 \tloss_epoch:-0.8572422206401825\u001b[0m\n",
      "tensor([0.0500, 0.2500, 0.8500,  ..., 0.0000, 1.0000, 0.0000], device='cuda:0')\n",
      "epoch:2 \t , step:0\t loss: -0.9766716957092285\n",
      "\u001b[34mepoch:2\t spearmanr:0.9373619468164834 \tloss_epoch:-0.9799049480965263\u001b[0m\n",
      "tensor([0.2500, 0.0500, 0.0000,  ..., 0.0000, 0.6500, 0.7000], device='cuda:0')\n",
      "valid epoch:2 \t , step:0\t loss: -0.8650296926498413\n",
      "\u001b[32mvalid ::: epoch:2\t spearmanr:0.8514293463190644 \tloss_epoch:-0.8604846715927124\u001b[0m\n",
      "tensor([0.3000, 0.0000, 0.5000,  ..., 0.0000, 0.1000, 0.0000], device='cuda:0')\n",
      "epoch:3 \t , step:0\t loss: -0.9859002232551575\n",
      "\u001b[34mepoch:3\t spearmanr:0.9471881993303107 \tloss_epoch:-0.9858836625751696\u001b[0m\n",
      "tensor([0.2500, 0.0500, 0.0000,  ..., 0.0000, 0.6500, 0.7000], device='cuda:0')\n",
      "valid epoch:3 \t , step:0\t loss: -0.8627180457115173\n",
      "\u001b[32mvalid ::: epoch:3\t spearmanr:0.8509037822947378 \tloss_epoch:-0.8606144964694977\u001b[0m\n",
      "tensor([0.0000, 0.7000, 0.0500,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "epoch:4 \t , step:0\t loss: -0.9886844158172607\n",
      "\u001b[34mepoch:4\t spearmanr:0.951297901009404 \tloss_epoch:-0.9867929847616899\u001b[0m\n",
      "tensor([0.2500, 0.0500, 0.0000,  ..., 0.0000, 0.6500, 0.7000], device='cuda:0')\n",
      "valid epoch:4 \t , step:0\t loss: -0.8653278350830078\n",
      "\u001b[32mvalid ::: epoch:4\t spearmanr:0.8534271459948908 \tloss_epoch:-0.8626692593097687\u001b[0m\n",
      "tensor([1.0000, 1.0000, 0.5000,  ..., 1.0000, 0.3000, 0.0000], device='cuda:0')\n",
      "epoch:5 \t , step:0\t loss: -0.9878467321395874\n",
      "\u001b[34mepoch:5\t spearmanr:0.9539700029978561 \tloss_epoch:-0.9876674727389687\u001b[0m\n",
      "tensor([0.2500, 0.0500, 0.0000,  ..., 0.0000, 0.6500, 0.7000], device='cuda:0')\n",
      "valid epoch:5 \t , step:0\t loss: -0.8641383051872253\n",
      "\u001b[32mvalid ::: epoch:5\t spearmanr:0.8534394156216584 \tloss_epoch:-0.8625369191169738\u001b[0m\n",
      "tensor([0.0000, 0.9000, 0.0000,  ..., 0.0000, 0.5000, 0.9000], device='cuda:0')\n",
      "epoch:6 \t , step:0\t loss: -0.9906189441680908\n",
      "\u001b[34mepoch:6\t spearmanr:0.9563530716941845 \tloss_epoch:-0.98882027205668\u001b[0m\n",
      "tensor([0.2500, 0.0500, 0.0000,  ..., 0.0000, 0.6500, 0.7000], device='cuda:0')\n",
      "valid epoch:6 \t , step:0\t loss: -0.8662858009338379\n",
      "\u001b[32mvalid ::: epoch:6\t spearmanr:0.8538130641601547 \tloss_epoch:-0.863849687576294\u001b[0m\n",
      "tensor([0.0000, 0.2500, 0.0000,  ..., 0.0000, 0.5000, 1.0000], device='cuda:0')\n",
      "epoch:7 \t , step:0\t loss: -0.9913581609725952\n",
      "\u001b[34mepoch:7\t spearmanr:0.9580964248399233 \tloss_epoch:-0.9899465284849468\u001b[0m\n",
      "tensor([0.2500, 0.0500, 0.0000,  ..., 0.0000, 0.6500, 0.7000], device='cuda:0')\n",
      "valid epoch:7 \t , step:0\t loss: -0.8662163019180298\n",
      "\u001b[32mvalid ::: epoch:7\t spearmanr:0.8534923118541459 \tloss_epoch:-0.8632771849632264\u001b[0m\n",
      "(54319, 3)\n",
      "(13580, 3)\n",
      "tensor([0.0500, 0.4000, 0.6000,  ..., 0.0000, 0.0000, 0.5500], device='cuda:0')\n",
      "epoch:0 \t , step:0\t loss: -0.5663536190986633\n",
      "\u001b[34mepoch:0\t spearmanr:0.7941662744184453 \tloss_epoch:-0.8195495009422302\u001b[0m\n",
      "tensor([1.0000, 0.8500, 1.0000,  ..., 1.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "valid epoch:0 \t , step:0\t loss: -0.8524940609931946\n",
      "\u001b[32mvalid ::: epoch:0\t spearmanr:0.8337551503893994 \tloss_epoch:-0.8473816514015198\u001b[0m\n",
      "tensor([0.0000, 0.9500, 0.0000,  ..., 0.0000, 0.0000, 0.3500], device='cuda:0')\n",
      "epoch:1 \t , step:0\t loss: -0.9368399381637573\n",
      "\u001b[34mepoch:1\t spearmanr:0.9110976516662118 \tloss_epoch:-0.9482552816993312\u001b[0m\n",
      "tensor([1.0000, 0.8500, 1.0000,  ..., 1.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "valid epoch:1 \t , step:0\t loss: -0.860088050365448\n",
      "\u001b[32mvalid ::: epoch:1\t spearmanr:0.8418382145353199 \tloss_epoch:-0.8550848424434662\u001b[0m\n",
      "tensor([0.8500, 0.0000, 0.5000,  ..., 0.3500, 0.0000, 0.0000], device='cuda:0')\n",
      "epoch:2 \t , step:0\t loss: -0.979994535446167\n",
      "\u001b[34mepoch:2\t spearmanr:0.9385751176934701 \tloss_epoch:-0.9798626836977506\u001b[0m\n",
      "tensor([1.0000, 0.8500, 1.0000,  ..., 1.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "valid epoch:2 \t , step:0\t loss: -0.8675697445869446\n",
      "\u001b[32mvalid ::: epoch:2\t spearmanr:0.8470233733229241 \tloss_epoch:-0.8595126748085022\u001b[0m\n",
      "tensor([0.3000, 1.0000, 0.0000,  ..., 0.4000, 0.0000, 1.0000], device='cuda:0')\n",
      "epoch:3 \t , step:0\t loss: -0.986687421798706\n",
      "\u001b[34mepoch:3\t spearmanr:0.9482453117595071 \tloss_epoch:-0.9855366490389171\u001b[0m\n",
      "tensor([1.0000, 0.8500, 1.0000,  ..., 1.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "valid epoch:3 \t , step:0\t loss: -0.8678423166275024\n",
      "\u001b[32mvalid ::: epoch:3\t spearmanr:0.8479668959165537 \tloss_epoch:-0.8586353898048401\u001b[0m\n",
      "tensor([0.0000, 0.1500, 0.3000,  ..., 0.0000, 0.0000, 1.0000], device='cuda:0')\n",
      "epoch:4 \t , step:0\t loss: -0.9872504472732544\n",
      "\u001b[34mepoch:4\t spearmanr:0.9522316846202921 \tloss_epoch:-0.9864532618146193\u001b[0m\n",
      "tensor([1.0000, 0.8500, 1.0000,  ..., 1.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "valid epoch:4 \t , step:0\t loss: -0.8687878847122192\n",
      "\u001b[32mvalid ::: epoch:4\t spearmanr:0.8494128759675349 \tloss_epoch:-0.861075097322464\u001b[0m\n",
      "tensor([0.0500, 0.0000, 0.8000,  ..., 0.0000, 0.5000, 0.6500], device='cuda:0')\n",
      "epoch:5 \t , step:0\t loss: -0.9884001612663269\n",
      "\u001b[34mepoch:5\t spearmanr:0.9544645783273702 \tloss_epoch:-0.9874657878750249\u001b[0m\n",
      "tensor([1.0000, 0.8500, 1.0000,  ..., 1.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "valid epoch:5 \t , step:0\t loss: -0.8695981502532959\n",
      "\u001b[32mvalid ::: epoch:5\t spearmanr:0.8493830989432206 \tloss_epoch:-0.86181281208992\u001b[0m\n",
      "tensor([0.0000, 0.8500, 0.7500,  ..., 0.0000, 0.0000, 0.9000], device='cuda:0')\n",
      "epoch:6 \t , step:0\t loss: -0.9908873438835144\n",
      "\u001b[34mepoch:6\t spearmanr:0.9574027745804368 \tloss_epoch:-0.9892000320710634\u001b[0m\n",
      "tensor([1.0000, 0.8500, 1.0000,  ..., 1.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "valid epoch:6 \t , step:0\t loss: -0.8699713945388794\n",
      "\u001b[32mvalid ::: epoch:6\t spearmanr:0.8497039914358918 \tloss_epoch:-0.8616689264774322\u001b[0m\n",
      "tensor([0.0500, 0.2000, 0.6000,  ..., 0.0000, 0.6500, 0.0000], device='cuda:0')\n",
      "epoch:7 \t , step:0\t loss: -0.991919219493866\n",
      "\u001b[34mepoch:7\t spearmanr:0.9596038400416356 \tloss_epoch:-0.9902184715396479\u001b[0m\n",
      "tensor([1.0000, 0.8500, 1.0000,  ..., 1.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "valid epoch:7 \t , step:0\t loss: -0.8708247542381287\n",
      "\u001b[32mvalid ::: epoch:7\t spearmanr:0.8498700792185365 \tloss_epoch:-0.861847597360611\u001b[0m\n",
      "(54319, 3)\n",
      "(13580, 3)\n",
      "tensor([0.3000, 1.0000, 0.0000,  ..., 0.0000, 0.5000, 0.5000], device='cuda:0')\n",
      "epoch:0 \t , step:0\t loss: -0.5793762803077698\n",
      "\u001b[34mepoch:0\t spearmanr:0.7913990739558425 \tloss_epoch:-0.8189544128744226\u001b[0m\n",
      "tensor([0., 0., 1.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "valid epoch:0 \t , step:0\t loss: -0.8447261452674866\n",
      "\u001b[32mvalid ::: epoch:0\t spearmanr:0.8380518896807906 \tloss_epoch:-0.8501874923706054\u001b[0m\n",
      "tensor([1.0000, 1.0000, 1.0000,  ..., 0.6500, 0.0000, 1.0000], device='cuda:0')\n",
      "epoch:1 \t , step:0\t loss: -0.9440648555755615\n",
      "\u001b[34mepoch:1\t spearmanr:0.9105587127633176 \tloss_epoch:-0.9482373814833792\u001b[0m\n",
      "tensor([0., 0., 1.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "valid epoch:1 \t , step:0\t loss: -0.8600121736526489\n",
      "\u001b[32mvalid ::: epoch:1\t spearmanr:0.847193190144997 \tloss_epoch:-0.861984658241272\u001b[0m\n",
      "tensor([0.0000, 0.0000, 0.0500,  ..., 0.0000, 0.0000, 0.4500], device='cuda:0')\n",
      "epoch:2 \t , step:0\t loss: -0.9808062314987183\n",
      "\u001b[34mepoch:2\t spearmanr:0.9383469489136168 \tloss_epoch:-0.9801631946312753\u001b[0m\n",
      "tensor([0., 0., 1.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "valid epoch:2 \t , step:0\t loss: -0.8597748875617981\n",
      "\u001b[32mvalid ::: epoch:2\t spearmanr:0.8511690982059639 \tloss_epoch:-0.8641037106513977\u001b[0m\n",
      "tensor([0.9000, 0.1500, 0.0000,  ..., 1.0000, 0.5000, 0.0000], device='cuda:0')\n",
      "epoch:3 \t , step:0\t loss: -0.9869442582130432\n",
      "\u001b[34mepoch:3\t spearmanr:0.948409543732825 \tloss_epoch:-0.9859522534044165\u001b[0m\n",
      "tensor([0., 0., 1.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "valid epoch:3 \t , step:0\t loss: -0.8585496544837952\n",
      "\u001b[32mvalid ::: epoch:3\t spearmanr:0.852237166044273 \tloss_epoch:-0.863624382019043\u001b[0m\n",
      "tensor([0.0000, 0.0000, 0.0000,  ..., 0.5000, 0.5500, 0.0000], device='cuda:0')\n",
      "epoch:4 \t , step:0\t loss: -0.9862506985664368\n",
      "\u001b[34mepoch:4\t spearmanr:0.9524192190972386 \tloss_epoch:-0.9868219290909014\u001b[0m\n",
      "tensor([0., 0., 1.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "valid epoch:4 \t , step:0\t loss: -0.8639101386070251\n",
      "\u001b[32mvalid ::: epoch:4\t spearmanr:0.8528373086723753 \tloss_epoch:-0.8652346789836883\u001b[0m\n",
      "tensor([1.0000, 0.0000, 0.1500,  ..., 0.1000, 0.6500, 0.0000], device='cuda:0')\n",
      "epoch:5 \t , step:0\t loss: -0.9889380931854248\n",
      "\u001b[34mepoch:5\t spearmanr:0.9546708293366222 \tloss_epoch:-0.9878271018203936\u001b[0m\n",
      "tensor([0., 0., 1.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "valid epoch:5 \t , step:0\t loss: -0.8618055582046509\n",
      "\u001b[32mvalid ::: epoch:5\t spearmanr:0.8539597669663768 \tloss_epoch:-0.8662513613700866\u001b[0m\n",
      "tensor([1.0000, 0.8000, 0.5000,  ..., 0.7500, 0.1000, 1.0000], device='cuda:0')\n",
      "epoch:6 \t , step:0\t loss: -0.9914395809173584\n",
      "\u001b[34mepoch:6\t spearmanr:0.9569823782785166 \tloss_epoch:-0.9890804243715186\u001b[0m\n",
      "tensor([0., 0., 1.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "valid epoch:6 \t , step:0\t loss: -0.8632062077522278\n",
      "\u001b[32mvalid ::: epoch:6\t spearmanr:0.8535397535612104 \tloss_epoch:-0.8651905953884125\u001b[0m\n",
      "tensor([0.7000, 0.4500, 0.0000,  ..., 0.5500, 0.0000, 0.5000], device='cuda:0')\n",
      "epoch:7 \t , step:0\t loss: -0.9915303587913513\n",
      "\u001b[34mepoch:7\t spearmanr:0.959193596429341 \tloss_epoch:-0.9901318769705924\u001b[0m\n",
      "tensor([0., 0., 1.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "valid epoch:7 \t , step:0\t loss: -0.861315906047821\n",
      "\u001b[32mvalid ::: epoch:7\t spearmanr:0.8543032755543715 \tloss_epoch:-0.866217291355133\u001b[0m\n",
      "(54319, 3)\n",
      "(13580, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46283/3397204245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvalid_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvaliding_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCHSIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mNET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputszlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputszlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddensz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTSZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m# print(NET)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data03/yrqUni/Workspace/QQB/zp/code/knowledge/KN_TEXTCNN.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, KNOW_NUM, inputszlist, hiddensz, outsz)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLPlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlplist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menhanceNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSENet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKNOW_NUM\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhiddensz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         self.finalMLP = nn.Sequential(\n\u001b[1;32m     81\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKNOW_NUM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhiddensz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data03/yrqUni/Workspace/QQB/zp/code/knowledge/KN_TEXTCNN.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_sz, ratio)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_unit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_unit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         )\n",
      "\u001b[0;32m/data02/yrqUni/anaconda3/envs/mix/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data02/yrqUni/anaconda3/envs/mix/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data02/yrqUni/anaconda3/envs/mix/lib/python3.7/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NET = Network(len(inputszlist), inputszlist, hiddensz=1536, outsz=OUTSZ).to(DEVICE)\n",
    "# print(NET)\n",
    "vid_embedding_list = []\n",
    "EPOCH = 8\n",
    "START_INFERTEN_EPOCH = 5\n",
    "\n",
    "for train_index, test_index in group_kfold.split(valid_df, y, groups):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_part, valid_part = valid_df.iloc[train_index], valid_df.iloc[test_index]\n",
    "#     train_part = query_expansion(train_part)\n",
    "    print(train_part.shape)\n",
    "    print(valid_part.shape)\n",
    "\n",
    "    training_data = ValidDataSet(valid_knowledgePool, train_part)\n",
    "    validing_data = ValidDataSet(valid_knowledgePool, valid_part)\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(training_data, batch_size=BATCHSIZE, shuffle=True, drop_last=False, num_workers=4)\n",
    "    valid_dataloader = DataLoader(validing_data, batch_size=BATCHSIZE, shuffle=False, drop_last=False, num_workers=4)\n",
    "    \n",
    "    NET = Network(len(inputszlist), inputszlist, hiddensz=1536, outsz=OUTSZ).to(DEVICE)\n",
    "    # print(NET)\n",
    "    optimizer = torch.optim.Adam(NET.parameters(), lr=1e-4, weight_decay=0)\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        data_enumerator = enumerate(train_dataloader)\n",
    "        valid_enumerator = enumerate(valid_dataloader)\n",
    "        loss_epoch = 0\n",
    "        train_ans = []\n",
    "        y_list = []\n",
    "        NET.train()\n",
    "        for step, (data1, data2, y) in data_enumerator:\n",
    "            optimizer.zero_grad()\n",
    "            data1 = tuple([data.to(DEVICE) for data in data1])\n",
    "            data2 = tuple([data.to(DEVICE) for data in data2])\n",
    "\n",
    "            y = y.float().to(DEVICE)\n",
    "            out1 = NET(data1)\n",
    "            out2 = NET(data2)\n",
    "            loss, preds = LOSS(out1, out2, y, return_sim=True)\n",
    "            # print(preds.shape)\n",
    "            # print(y.shape)\n",
    "            # preds = cosine(out1, out2)\n",
    "\n",
    "            train_ans.extend(list(preds.detach().cpu().numpy()))\n",
    "            y_list.extend(list(y.detach().cpu().numpy()))\n",
    "            loss_epoch += loss.item()\n",
    "            if step % 50 == 0:\n",
    "                print(y)\n",
    "                print(f\"epoch:{epoch} \\t , step:{step}\\t loss:\", loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch >= START_INFERTEN_EPOCH:\n",
    "            vid_emb = inference_one_epoch(test_dataloader, NET)\n",
    "            vid_embedding_list.append(vid_emb)\n",
    "\n",
    "        if not ALLIN:\n",
    "            spearmanr = scipy.stats.spearmanr(train_ans, y_list).correlation\n",
    "            info = f\"epoch:{epoch}\\t spearmanr:{spearmanr} \\tloss_epoch:{loss_epoch / len(train_dataloader)}\"\n",
    "            print(colored(info, 'blue'))\n",
    "\n",
    "        loss_epoch = 0\n",
    "        val_ans = []\n",
    "        y_list = []\n",
    "        if not ALLIN:\n",
    "            NET.eval()\n",
    "            for step, (data1, data2, y) in valid_enumerator:\n",
    "                optimizer.zero_grad()\n",
    "                data1 = tuple([data.to(DEVICE) for data in data1])\n",
    "                data2 = tuple([data.to(DEVICE) for data in data2])\n",
    "\n",
    "                y = y.float().to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    out1 = NET(data1)\n",
    "                    out2 = NET(data2)\n",
    "                    # print(final_emb1)\n",
    "                    loss, preds = LOSS(out1, out2, y, return_sim=True)\n",
    "                    val_ans.extend(list(preds.detach().cpu().numpy()))\n",
    "                    y_list.extend(list(y.detach().cpu().numpy()))\n",
    "                    loss_epoch += loss.item()\n",
    "                if step % 10 == 0:\n",
    "                    print(y)\n",
    "                    print(f\"valid epoch:{epoch} \\t , step:{step}\\t loss:\", loss.item())\n",
    "\n",
    "            spearmanr = scipy.stats.spearmanr(val_ans, y_list).correlation\n",
    "            info = f\"valid ::: epoch:{epoch}\\t spearmanr:{spearmanr} \\tloss_epoch:{loss_epoch / len(valid_dataloader)}\"\n",
    "            print(colored(info, 'green'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde342b-9c46-4a54-96c2-aafb53c9e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add CNN epoch:10\t spearmanr:0.8386718699764899 \tloss_epoch:-0.8510410019329616\n",
    "# add trans epoch:4\t spearmanr:0.8474135897243554 \tloss_epoch:-0.8593714322362628\n",
    "# add roberta ich\n",
    "# SENet BN epoch:5\t spearmanr:0.8484653604794136 \tloss_epoch:-0.8606224996703011\n",
    "# BN SENet BN epoch:5\t spearmanr:0.8519170972032961 \tloss_epoch:-0.8641518013817924\n",
    "# BN SENet epoch:5\t spearmanr:0.8504814605848442 \tloss_epoch:-0.86269554070064\n",
    "# w/o SENet  epoch:5\t spearmanr:0.8484274529439838 \tloss_epoch:-0.8610541905675616\n",
    "# +++++ remove all dropout ｜ add BN  epoch:5\t spearmanr:0.8513013300677326 \tloss_epoch:-0.8634524089949471\n",
    "# BN SENet BN epoch:5\t spearmanr:0.8518227148627213 \tloss_epoch:-0.8639950922557286\n",
    "# MSE loss epoch:5\t spearmanr:0.8301060927771783 \tloss_epoch:0.04277116911751883\n",
    "\n",
    "# 2000 samples\n",
    "# epoch:5\t spearmanr:0.8314429985271974 \tloss_epoch:-0.8408079147338867\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "667ed1b3-fdde-412d-b8ee-466083f5b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mtotal 9 embeddings\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 43027/43027 [00:11<00:00, 3744.34it/s]\n"
     ]
    }
   ],
   "source": [
    "vid_embedding = merge_embedding(vid_embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12b9a7-0deb-4420-921f-10ac8937370e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea613fbe-3fa1-43f5-8f37-23eb575a1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'two_stage/5foldSP_addyrq_mean'\n",
    "import os\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "output_json = path + '/result.json'\n",
    "output_zip = path + '/result.zip'\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "from zipfile import ZIP_DEFLATED, ZipFile\n",
    "\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(vid_embedding, f)\n",
    "with ZipFile(output_zip, 'w', compression=ZIP_DEFLATED) as zip_file:\n",
    "    zip_file.write(output_json)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9799a-751e-427d-9349-4021b54c9311",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mix",
   "language": "python",
   "name": "mix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
